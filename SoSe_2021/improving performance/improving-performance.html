<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Advanced R for Econometricians</title>
    <meta charset="utf-8" />
    <meta name="author" content="Martin C. Arnold" />
    <meta name="date" content="2020-01-21" />
    <link href="improving-performance_files/remark-css/default.css" rel="stylesheet" />
    <link href="improving-performance_files/font-awesome/css/fontawesome-all.min.css" rel="stylesheet" />
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
    </script>
    <style>
    .mjx-mrow a {
      color: black;
      pointer-events: none;
      cursor: default;
    }
    </style>
    
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"
      integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
      crossorigin="anonymous"></script>
    <link rel="stylesheet" href="../assets/sydney-fonts.css" type="text/css" />
    <link rel="stylesheet" href="../assets/sydney.css" type="text/css" />
    <link rel="stylesheet" href="../assets/title_slides.css" type="text/css" />
    <link rel="stylesheet" href="../xaringan_files/custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






class: title-slide title-performance center middle

# Advanced R for Econometricians
## Improving Performance
### Martin C. Arnold

---
class: left
## Improving Performance


```r
# packages needed
library(tidyverse) # for bench package
library(parallel)
library(MonteCarlo)
```

&lt;style&gt;
  strong {
    color:#e64626;
  }
  
&lt;/style&gt;

&lt;blockquote style ="margin-top:10%;"&gt;
We should forget about small efficiencies, say about 97% of the time: premature optimisation is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.
.right[&amp;mdash; &lt;cite&gt;Donald Knuth&lt;/cite&gt;]
&lt;/blockquote&gt;

---
## Improving Performance &amp;mdash; Overview

Once profiling has revealed a bottleneck, we need to make it faster. In the following we will discuss some techniques that are broadly useful:

1. **Look for existing solutions.**

    Searching google can save you lots of time. The benefit of finding a faster (but possibly suboptimal) solution on your own may be small.

2. **Do as little as possible. Be precise.**

    Using optimized functions tailored for specific tasks often makes a big difference.

3. **Vectorise your code.**

    This often relates to avoiding loops and the associated inefficiencies.

4. **Parallelise (if possible).**

    Using multiple CPU cores in parallel is enormously helpful when the computation involves a large number of tasks that may be solved independently.

---
## 1. Look for Existing Solutions

There's a good chance that someone has already tackled the same problem.

- Find out if there is a [Cran Task View](https://cran.rstudio.com/web/views/) related to your problem and search the packages listed there

- Limit your search to packages that depend on the `Rcpp` package (and thus likely implement high-performant C++ code). This is easily done by checking reverse dependencies of [Rcpp](https://cran.r-project.org/web/packages/Rcpp/).

- Check if a question related to your problem has been asked on [stackoverflow](https://stackoverflow.com/). Narrow the search by using suitable tags, e.g., `[r]`, `[performance]`.

- Google. For R-related results, use [rseek](https://rseek.org/).

**Record all solutions that you find not just those that immediately appear to be faster!**

- Some solutions might be initially slower but are are easier to optimize and thus end up being faster. 

- Sometimes combining the fastest parts from different approaches is helpful. 

---
## Improving Performance &amp;mdash; Golden Rule

Before we continue, we introduce a rule we have already followed implicitly: the **golden rule of R programming**:

  ***Access the underlying C/FORTRAN routines as quickly as possible; the fewer functions calls required to achieve this, the better.***

  ***(Lovelace and Gillespie, 2016)***

![:image 30%](../img/goldenR.png)

The techniques discussed next follow this paradigm. If this is not enough, we still may rewrite (some of) the code in C++ using the Rcpp package (discussed in the next chapter).

---
## 2. Do as Little as Possible

.smaller[

If you cannot find existing solutions, start by reducing your code to the bare essentials.

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Exercise: coercion of inputs / robustness checks


```r
X &lt;- matrix(1:1000, ncol = 10)
Y &lt;- as.data.frame(X)
```

Which approach is faster and why?


```r
apply(X, 1, sum)
apply(Y, 1, sum)
```

]]

---
## 2. Do as Little as Possible

.smaller[

A function is faster if it has less to do &amp;mdash; obvious but often neglected! 

**Use functions tailored to specific input and output types / specific problems!**

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Exercise: coercion of inputs / robustness checks &amp;mdash; ctd.

Which approach is faster and why?


```r
rowSums(X)
apply(X, 1, sum)
```

]]

---
## 2. Do as Little as Possible

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Exercise: searching a vector

What is the fastest way to check if `10` is an element of `1:100`?

]]

---
## 2. Do as Little as Possible

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Exercise: Linear Regression &amp;mdash; computation of `\(SE(\widehat\beta)\)`

Which approach is faster and why? Name disadvantages of the fastest one.


```r
set.seed(1)
X &lt;- matrix(rnorm(100), ncol = 1)
Y &lt;- X + rnorm(100)

a &lt;- function() {
  coefficients(summary(lm(Y ~ X - 1)))[1, 2]  
}

b &lt;- function() {
  fit &lt;- lm.fit(X, Y)
  c(
    sqrt(1/(length(X)-1) * sum(fit$residuals^2) * solve(t(X) %*% X))
    )
}
```

]]

---
## 2. Do as Little as Possible

.smaller[

Be as explicit as possible.

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: method dispatch takes time


```r
x &lt;- runif(1e2)

bench::mark(
  mean(x),
  mean.default(x)
)
```

```
## # A tibble: 2 x 6
##   expression           min   median `itr/sec` mem_alloc `gc/sec`
##   &lt;bch:expr&gt;      &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;
## 1 mean(x)           2.64µs   3.07µs   297320.    22.7KB      0  
## 2 mean.default(x)   1.13µs   1.35µs   659992.        0B     66.0
```
]]

---
## 2. Do as Little as Possible

.smaller[
Benchmark against alternatives that rely on primitive functions.
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: primitives are faster

```r
bench::mark(
  mean.default(x),
  sum(x)/length(x)
)
```

```
## # A tibble: 2 x 6
##   expression            min   median `itr/sec` mem_alloc `gc/sec`
##   &lt;bch:expr&gt;       &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;
## 1 mean.default(x)    1.15µs   1.38µs   658724.        0B        0
## 2 sum(x)/length(x)    432ns    526ns  1637111.        0B        0
```
]]

---
## 2. Do as Little as Possible &amp;mdash; Exercises

.smaller[
1. Can you come up with an even faster implementation of `b()` in the linear regression example?


2. What’s the difference between `rowSums()` and `.rowSums()`?

3. `rowSums2()` is an alternative implementation of `rowSums()`. Is it faster for the input `df`? Why?
    
    ```r
    rowSums2 &lt;- function(df) {
      out &lt;- df[[1L]]
      if (ncol(df) == 1) return(out)
        for (i in 2:ncol(df)) {
          out &lt;- out + df[[i]]
        }
      out
    }
        
    df &lt;- as.data.frame(
      replicate(1e3, sample(100, 1e4, replace = TRUE))
    )
    ```

]

---
## 2. Do as Little as Possible &amp;mdash; Case Study

.smaller[

Imagine you want to compute the bootstrap distribution of a sample correlation coefficient using `cor_df()` and the data in the example below. 

Given that you want to run this many times, how can you make this code faster? 


```r
n &lt;- 1e6
df &lt;- data.frame(a = rnorm(n), b = rnorm(n))
    
cor_df &lt;- function(df, n) {
  i &lt;- sample(seq(n), n, replace = TRUE)
  cor(df[i, , drop = FALSE])[2, 1]
}
```

]

---
## 3. Vectorise your code

.smaller[

**What is vectorisation?** 

- Vectorisation is the process of converting an algorithm from operating on a single value at a time to operating on a **set of values** (like a vector) at one time.

- It makes problems simpler: code that operates on vectors instead of single entries of an array is often less complex to write.

**Why is it efficient?** 

(on the lowest level where functions map closely to processor instructions)

- Your computer is designed to run vectorised operations: a vectorised function may run multiple operations from a single instruction which is faster than sending individual instructions for each operation.  

]

---
## 3. Vectorise your code

.smaller[

**Why is vectorisation efficient? &amp;mdash; ctd.** 

Many languages (including R) work on arrays that are stored in the memory in [column-major order](https://en.m.wikipedia.org/wiki/Row-_and_column-major_order).

![:image 60%](../img/rowcolumnarrays.jpg)

Disregarding these patterns by writing code which operates on `\(1\times1\)` vectors thus means 'fighting the language'.

]

---
## 3. Vectorise your code

.smaller[
**Why is vectorisation efficient? &amp;mdash; ctd. **

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: vector addition

`$$\begin{pmatrix} 1 \\ 2\\ 3 \end{pmatrix}+\begin{pmatrix} 4 \\ 5\\ 6 \end{pmatrix} = \begin{pmatrix} 5 \\ 7\\ 9 \end{pmatrix}$$`

- **Slow:** three instructions, three operations

    - *add `\(\ 1\)` and `\(\ 4\)`*
    - *add `\(\ 2\)` and `\(\ 5\)`*
    - *add `\(\ 3\)` and `\(\ 6\)`*

- **Fast:** one instruction, one vectorised operation:

    - *add `\(\begin{pmatrix} 1 \ 2 \  3 \end{pmatrix}'\)` and `\(\begin{pmatrix} 4 \ 5 \  6 \end{pmatrix}'\)`*

    - The three additions are effectively done *in parallel*.
]]

???

Aagain, less related to R itself but to what the CPU does.


---
## 3. Vectorise your code

.smaller[
**What does it mean to write 'vectorised' code in R?**

Remember that there are no 'real' scalars in R. Everything that looks like a scalar is actually a `\(1\times1\)` vector.

![:image 23%](../img/atomic_vectors.png)


```r
# otherwise this shouldn't work:
1[1]
```

```
## [1] 1
```
Thus 'scalar' operations work on vector *elements* which is (often) needlessly cumbersome.
]

---
## 3. Vectorise your code

.smaller[

**What does it mean to write 'vectorised' code in R?**

By 'vectorised' we mean that the function works on vectors, i.e., performs vector arithmetic and calls functions which work on vectors.

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: scalar vs. vectorised computation of euclidean norm

.pull-left[

```r
L2_scalar &lt;- function(x) {
  out &lt;- numeric(1)
  for(i in 1:length(x)) {
    out &lt;- x[i] * x[i] + out 
  }
  return(sqrt(out))
}
```
]

.pull-right[

```r
L2_vec &lt;- function(x) {
 return(
   sqrt(
     sum(x * x)
     )
   )
}
```
]]

]

---
## 3. Vectorise your code

.smaller[

**What does it mean to write 'vectorised' code in R?**

By 'vectorised' we mean that the function works on vectors, i.e., performs vector arithmetic and calls functions which work on vectors.

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: scalar vs. vectorised computation of euclidean norm

```r
bench::mark(
  L2_scalar(1:1e4),
  L2_vec(1:1e4)
)
```

```
## # A tibble: 2 x 6
##   expression              min   median `itr/sec` mem_alloc `gc/sec`
##   &lt;bch:expr&gt;         &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;
## 1 L2_scalar(1:10000)  713.1µs  775.8µs     1236.    4.11MB      0  
## 2 L2_vec(1:10000)      40.3µs   73.1µs    12708.   78.22KB     16.9
```
]]

---
## 3. Vectorise your code &amp;mdash; Some Principles

.smaller[

- **Do not torture the interpreter**

    Vectorisation reduces the amount of intepreting R has to do: Validating that `x &lt;- 1L` and `y &lt;- 1:100L` are of type `integer` is equally expensive. Checking that each element of `y` is an integer is not!   

- **Use functions from base R that work on vectors**

    Vectorisation may avoid loops in R. Loops in many 'vectorised' R functions are carried out in C and thus are much faster. 

    The following functions are prominent examples:

    `rowSums()`, `colSums()`, `rowMeans()`, `colMeans()`, `cumsum()`, `diff()`

- **Use matrix algebra**

    Most matrix operations that involve loops are executed by highly optimized external Fortran and C libraries. See, e.g., [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms).

]

---
## 3. Vectorise your code

.smaller[

**Are `for()` loops slower than `*apply()`?**

`for()` loops have the reputation of being slow(er than `*apply()`). This is widespread wisdom and not generally true:

- `for()` is *not* slow if we iterate over data and apply a (non-vectorised) function. Execution time is comparable to `*apply()`.

- Loops are often used in an *inefficient* manner: 

    Growing an object and improper initialisation means overhead due to **growing / copying** in every iteration. 
    
    Getting used to `*apply()` is a good practice as it prevents us from doing silly stuff like the above.
    
**What if I need to use `for` / how to properly write a `for` loop?**

Allocate the required storage before the loop and *fill* the object!

]

---
## 3. Vectorise your code

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: avoid growing objects



![:image 80%](../img/growing_objects.png)
]


---
## 3. Vectorise your code

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: good and bad `for()` loops
.pull-left[.smaller[

```r
# Bad: c()/cbind()/rbind()
rw_bad &lt;- function(N) {
  set.seed(1)
  out &lt;- rnorm(1)
  
  for(i in 2:N) {
   out &lt;- c(out, 
            out[i-1] + rnorm(1))
  }
  
  return(out)
}
```
]]
.pull-right[
![:image 48%](../img/bad.png)
]]

---
## 3. Vectorise your code

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: good and bad `for()` loops
.pull-left[.smaller[

```r
# Good: proper initialisation/iteration
rw_good &lt;- function(N) {
  set.seed(1)
  out &lt;- vector("double", N)
  
  out[1] &lt;- rnorm(1)
  for(i in 2:N) {
   out[i] &lt;- rnorm(1) + out[i-1]
  }
  
  return(out)
}
```
]]
.pull-right[
![:image 48%](../img/good.png)
]]

???

---
## 3. Vectorise your code

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: good and bad `for()` loops

```r
bench::mark(rw_good(1e4), 
            rw_bad(1e4))
```

```
## # A tibble: 2 x 6
##   expression          min   median `itr/sec` mem_alloc `gc/sec`
##   &lt;bch:expr&gt;     &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;
## 1 rw_good(10000)   15.2ms   22.6ms     39.3     24.8MB     21.6
## 2 rw_bad(10000)     346ms  351.5ms      2.85   406.3MB     27.0
```
]

---
## 3. Vectorise your code

**Are `for()` loops slower than the `*apply()` functions?**

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: `for()` vs. `apply()`

```r
X &lt;- matrix(rnorm(1e4), ncol = 1e4)

colmax &lt;- function(x) {
  out &lt;- numeric(ncol(x))
  
  for(j in 1:ncol(x)) {
    out[j] &lt;- max(x[,j])
  }
  
  return(out)
}
```
]

---
## 3. Vectorise your code

**Are `for()` loops slower than the `*apply()` functions?**

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: `for()` vs. `apply()` &amp;mdash; ctd. 

```r
bench::mark(
  colmax(X),
  apply(X, 2, max)
)
```

```
## # A tibble: 2 x 6
##   expression            min   median `itr/sec` mem_alloc `gc/sec`
##   &lt;bch:expr&gt;       &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;
## 1 colmax(X)          5.75ms   6.37ms     156.     4.34MB     55.0
## 2 apply(X, 2, max)   9.05ms  10.27ms      96.6  312.73KB     55.2
```
]

---
## 3. Vectorise your code &amp;mdash; Exercises

1. Compare the speed of `apply(X, 1, sum)` with the vectorised `rowSums(X)` for varying sizes of the square matrix `X` using `bench::mark()`. Consider the dimensions `1`, `1e1`, `1e2`, `1e3`, `0.5e4` and `1e5`. Visualize the results using a violin plot.

2. 

    (a) How can you vectorise the computation of a weighted sum?

    (b) How can you use `crossprod()` to compute that same sum? 

    (c) Compare performance of the approaches in (a) and (b)

3.  Find an approach to compute column maxima of a numeric matrix `X` using a `*apply()`-function which outperforms `apply(X, 2, max)`.

---
## 3. Vectorise your code &amp;mdash; Case Study

.smaller[
**Case Study: Monte Carlo Integration**

Suppose you are interested in computing `$$A = \int_0^1 x^2 \mathrm{d}x$$` using Monte Carlo (MC) integration. Consider the following algorithm which operates on `\(1\times 1\)` vectors in a `for()` loop:

1. *initialize `counts`*

2. `for i in 1:N`

    *Generate `\(\ U_i \sim i.i.d. \mathcal{U}(0,1);\ i=1,2\)`.*
    
    *If `\(\ U_2 &lt; U_1^2\)`, then* `counts = counts + 1`

3. *end* `for`

4. *Estimate `\(A\)` by* `counts/N`
]

---
## 3. Vectorise your code &amp;mdash; Case Study

**Case Study: Monte Carlo Integration**

1. 
    
    (a) Implement the algorithm in a function `A_loop(N)`. A should use a loop to compute the MC estimate of `\(A\)` with `N` MC iterations.
    
    (b) Estimate `\(A\)` using `A_loop(N)` with 5e5 MC iterations.

2. 

    (a) Define a function `A_vec(N)` which implements the MC algorithm using a vectorised approach.
    
    (b) Compare `A_vec(N)` and `A_loop(N)` in a microbenchmark for `N = 5e5` iterations.
    
    (c) How does the difference in performance relate to `N`?

---
## 4. Parallelisation &amp;mdash; FAQs

**What is parallelisation?**

- In the simplest sense, parallelisation means the simultaneous use of multiple hardware resources for solving a computational problem.

- In the following we refer to parallelisation as the distribution of computing tasks over several CPUs in a shared memory system (a system where multiple CPUs access the same memory).

**Does R perform parallelisation by default?**

No. The 'stock' R version on CRAN is a single-threaded program, i.e., R does not benefit from modern multi-threaded CPUs, multiple cores let alone multiple CPUs.

---
## 4. Parallelisation &amp;mdash; FAQs

**Is it complicated to parallelise code?**

- In general, yes. Fortunately, there are R packages for the most common platforms that do the work for us. These packages facilitate the process *substantially*. 

- In most cases, we don't even have to adjust our code in order for it to be executable in parallel.

**Which tasks can be done in parallel?**

- In principle, all operations that can be executed independently. 

- Parallel computing is often used in MC simulations which require computation of a large number of homogeneous and independent tasks.

**Is parallelisation always faster than 'serial' computation?**

No. The cost of managing the computations (the *overhead*) may offset or even surpass time savings gained by parallel computation. This occurs, e.g., when a small number of simple tasks is solved in parallel. We will discuss an example.

---
## 4. Parallelisation &amp;mdash; the `parallel` package

The `parallel` package (comes with base R) is a good starting point for parallel computing.


```r
# detect numer of cores (including logical cores)
library(parallel)
detectCores()
```

```
## [1] 8
```

The notebook used to run this script has a 2,3 GHz Intel Core i5 processor with 4 physical CPUs and supports hyper-threading, i.e., each CPU has 2 logical cores.


```r
# detect numer of physical cores
detectCores(logical = F)
```

```
## [1] 4
```

Note that the behavior of `detectCores()` is platform specific and not always reliable, see `?detectCores()`.

---
## 4. Parallelisation - `parallel::mclapply()`

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: send them cores to sleep (on MacOS)

```r
r &lt;- mclapply(1:8, function(i) Sys.sleep(20), mc.cores = 8)
```

![:image 50%](../img/cpumonitor.png)
]]

---
## 4. Parallelisation - `parallel::mclapply()`

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: send them cores to sleep (on MacOS) &amp;mdash; ctd.
What happened?

- The R session (the main Process) opened 8 sub-processes, so-called *forks* (labeled `rsession` in the activity monitor)

- The forks operate independently from the main process (but they 'know' about each other). Each fork operates on a separate core.

- `mclapply()` passes a call of `function(i) Sys.sleep(20)` to each fork. Computation (or rather doing nothing for 20 seconds) now proceeds in parallel.

- After 20 seconds the results are gathered in `r` and all forks are killed. Note that `r` is just a list with `NULL` entries.
]]
  
---
## 4. Parallelisation - `parallel::mclapply()`

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: send them cores to sleep (Windows)  

An alternative on Windows is to use `parallel::parLapply()`. The setup is somewhat more complex.


```r
# set number of cores to be used
cores &lt;- detectCores(logical = TRUE) - 1
# initialize cluster
cl &lt;- makeCluster(cores)
# run lapply in parallel
parLapply(cl, 1:8, function(i) Sys.sleep(20))
# stop cluster
stopCluster(cl)
```

]]
              
---
## 4. Parallelisation

**"Embarrassingly parallel operations"**

![:image 80%](../img/parallelization.png)
![:source_url](https://bit.ly/2OX6e3e)

---
## 4. Parallelisation

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: parallelised bootstrapping

Remember the function `cor_df()` from the case study in *2. Do as Little as Possible*. A faster approach, `cor_df2()`, is given  below.


```r
n &lt;- 1e4
df &lt;- data.frame(a = rnorm(n), b = rnorm(n))

cor_df2 &lt;- function(x, n) {
  i &lt;- sample.int(n, n, replace = T)
  cor(df$a[i], df$b[i])
}
```
]

???

`cor_df2()` would be even faster if `df` was a `matrix` object.

```
function(df, n) {
  i &lt;- sample(1:n, n, replace = T)
  m &lt;- df[i,]
  cor(m)[2, 1]
}
```

---
## 4. Parallelisation

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: parallelised bootstrapping &amp;mdash; ctd.

Calling `cor_df2()` a large number of times is time consuming. 


```r
# serial computation
s_ser &lt;- system.time(
  r_ser &lt;- lapply(1:1e5, function(x) cor_df2(df, n))
)
s_ser
```

```
##     
##   user  system  elapsed 
## 41.258   7.497  48.835
```
]

---
## 4. Parallelisation

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: parallelised bootstrapping &amp;mdash; ctd.

Luckily, it is straightforward to bootstrap in parallel using `mclapply()`.


```r
# parallel computation
s_par &lt;- system.time(
  r_par &lt;- mclapply(1:1e5, function(x) cor_df2(df, n), mc.cores = 8)
)
s_par
```

```
##     
##   user  system  elapsed 
## 38.343   7.995   16.343
```

Note that using 8 cores in my case is (only) ~5 times faster than the serial approach due to overhead. 
]

---
## 4. Parallelisation

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: too much overhead

Parallelisation may be inefficient:


```r
system.time(mclapply(1:1e5, sqrt, mc.cores = 8))
```


```
## 
## ##   user  system elapsed 
## ##  0.107   0.110   0.062 
## 
```


```r
system.time(lapply(1:1e5, sqrt))
```


```
## 
## ##   user  system elapsed 
## ##  0.034   0.001   0.035 
## 
```

This is because the cost of the individual computations is low, and additional work is needed to send the computation to the different cores and to collect the results: The overhead exceeds the time savings. 
]]

---
## 4. The `MonteCarlo` Package

**Why is worthwhile to get used to such packages? **

- Running MC simulations is an everyday task in econometrics: simulations are routinely used for approximating quantities which cannot be derived analytically, e.g., finite sample properties of a test statistic.

- MC simulation is also an essential tool in statistical programming because we often write functions that expect random input and/or produce random output 

    *Testing whether the impementation is correct amounts to checking if the statistical properties of the output are as expected, using a large number of samples*

- Often, the effort to write the simulation exceeds the effort to implement the function by a multiple

---
## 4. The `MonteCarlo` Package

The `MonteCarlo` package streamlines the process of writing MC simulations. 

Useful features are:

- Simulation over a user-defined parameter grid

- Automatic parallelisation using the `snow` package

- Compilation of the results in an array. Generation of customisable LaTeX tables is possible.

The package is available on `CRAN`. 

Development version @ https://github.com/FunWithR/MonteCarlo

---
## 4. The `MonteCarlo` Package

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: Unit Root test
Consider the DGP `$$y_t = \rho y_{t-1} + \epsilon_t, \ \ \epsilon_t\sim N(0,1)$$` with `\(y_0 = 0\)` and `\(\rho = 1.\)` It is a well known result that the limit of the `\(t\)`-ratio `$$t = \frac{\hat\rho-1}{SE(\hat\rho)}$$` can be written as a functional of [Wiener processes](https://en.wikipedia.org/wiki/Wiener_process),

$$ t \Rightarrow \frac{[W(1)]^2 -1}{2\sqrt{\int_0^1[W(r)]^2 dr}}.$$

The asymptotic distribution of `\(t\)` is non-normal and its quantiles cannot be derived analytically.
]]

---
## 4. The `MonteCarlo` Package

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: Unit Root test &amp;mdash; ctd.
We use the `MonteCarlo` package to simulate the limit distribution of `\(t\)`.


```r
library(MonteCarlo)
```


```r
sim &lt;- function(rho, Time) {
  Y &lt;- matrix(NA_real_, ncol = 1, nrow = Time); Y[1,1] &lt;- rnorm(1)
  
  for(t in 2:Time) { Y[t, 1] &lt;- rho * Y[t-1, 1] + rnorm(1) }

  y &lt;- Y[-1, , drop = FALSE]
  ylag &lt;- Y[-Time, , drop = FALSE]
  
  fit &lt;- lm.fit(ylag, y)
  rho &lt;- fit$coefficients
  res &lt;- fit$residuals
  sigma &lt;- sqrt(1/(Time-2) * t(res) %*% res * 1/ (t(ylag) %*% ylag))

  return( list("t" = (rho[[1]] - 1)/sigma) )

}
```
]]

---
## 4. The `MonteCarlo` Package

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: Unit Root test &amp;mdash; ctd.


```r
# setup list of parameters
params &lt;- list("rho" = 1, 
               "Time" = 1000)

# run parallelised MC simulation over parameter grid
r &lt;- MonteCarlo(
  sim,
  nrep = 50000,
  param_list = params, 
  ncpus = parallel::detectCores()
)
```
We transform the simulated test statistics to a vector.

```r
t_sim &lt;- apply(r$result$t, 3, c)
```
]]

---
## 4. The `MonteCarlo` Package

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: Unit Root test &amp;mdash; ctd.

Estimates of the finite sample quantiles are close to the quantiles of the corresponding asymptotic Dickey-Fuller distribution


```r
quantile(t_sim, c(0.05, 0.1, 0.5, 0.9, 0.95))
```

```
##         5%        10%        50%        90%        95% 
## -1.9472738 -1.6124977 -0.5044926  0.8846489  1.2702348
```

```r
fUnitRoots::qunitroot(c(0.05, 0.1, 0.5, 0.9, 0.95), trend = "nc")
```

```
## [1] -1.9408468 -1.6167526 -0.4999276  0.8877673  1.2836012
```

We compare with the Gaussian distribution.


```r
plot(density(t_sim), main = "Dickey-Fuller Distribution")
curve(dnorm(x), from = -4, to = 4, add = T, lty = 2, col = "red")
```
]]

---
## 4. The `MonteCarlo` Package

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: Unit Root test &amp;mdash; ctd.
&lt;img src="improving-performance_files/figure-html/unnamed-chunk-41-1.png" style="display: block; margin: auto;" /&gt;
]]

---
## 4. The `MonteCarlo` Package
.smaller[
It is a great benefit to work with the package if you want to run the simulation for several parameter combinations. 

.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: Power of DF-Test

We write a simple wrapper for `sim()` which returns a logical value for rejection at 5% using the asymptotic critical value.


```r
# asymptotic 5% critical value of the corresponding DF-distribution
crit &lt;- fUnitRoots::qunitroot(0.05, trend = "nc")

# rejection?
sim_pow &lt;- function(rho, Time) {
  return(
   list("t" = sim(rho, Time)[[1]] &lt; crit)   
  )
}
```
]]

---
## 4. The `MonteCarlo` Package

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: Power of DF-Test &amp;mdash; ctd.

We consider all combination for a sequence of alternatives and three different sample sizes.


```r
# setup list of parameters (yielding 33 parameter constellations)
params &lt;- list("rho" = seq(0.75, 1, 0.025), 
               "Time" = c(50, 100, 200))

# run parallelised MC simulation over parameter grid
r &lt;- MonteCarlo(
  sim_pow,
  nrep = 5000,
  param_list = params, 
  ncpus = parallel::detectCores())
```
]]

---
## 4. The `MonteCarlo` Package

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: Power of DF-Test &amp;mdash; ctd.
Content of a `MonteCarlo` object is easily transformed and visualised using `tidyverse` functions.


```r
library(dplyr)
library(ggplot2)

tbl &lt;- r %&gt;% MakeFrame() %&gt;% tbl_df() %&gt;% 
  group_by(rho, Time) %&gt;% summarise(power = mean(t))

ggplot(tbl) + 
  geom_line(aes(x = rho, y = power, col = factor(Time)))
```
]]

---
## 4. The `MonteCarlo` Package

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: Power of DF-Test &amp;mdash; ctd.
&lt;img src="improving-performance_files/figure-html/unnamed-chunk-45-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]]

---
## 4. The `MonteCarlo` Package

.smaller[
.blockquote.exercise[
#### <i class="fas  fa-desktop "></i> Example: Power of DF-Test &amp;mdash; ctd.
Conversion of the results into a `\(\LaTeX\)` table is particularly useful.

```r
# generate LaTeX table from simulation results
MakeTable(r, rows = "Time", "rho")
```
![:image 80%](../img/latex_MakeTable.png)
]]

&lt;!-- --- --&gt;
&lt;!-- ## Asynchronous Programming --&gt;

&lt;!-- ![:image 60%](../img/futures.jpg) --&gt;

&lt;!-- --- --&gt;
&lt;!-- ## Futures --&gt;

&lt;!-- **What are futures?** --&gt;

&lt;!-- - A *future* or *promise* is a construct which describes an object that acts as a proxy for some value which will be available at some point in the future. --&gt;

&lt;!-- - A future allows execution of code until the point where its value is actually needed.  --&gt;

&lt;!-- **Why are they useful?** --&gt;

&lt;!-- - Futures can by evaluated asynchronously in a separate process (possibly on a different core/CPU), i.e. they do not block the 'main' process which is available for further processing so that other computations can run in the meantime --&gt;

&lt;!-- - Futures thus allow faster and more efficient code. They are a simple yet powerful construct for parallel computing and distributed processing. --&gt;

&lt;!-- **Is it difficult to work with futures?** --&gt;

&lt;!-- It depends. We will restrict ourselves to functions which have been *futurised*, i.e. (re)implemented using functions from the `future` package. --&gt;

&lt;!-- --- --&gt;
&lt;!-- ## The `future.apply` Package --&gt;

&lt;!-- ```{r} --&gt;
&lt;!-- library(future.apply) --&gt;
&lt;!-- ``` --&gt;

&lt;!-- The `future.apply` package provides a *futurised* version of the base R `*apply()` function family. --&gt;

&lt;!-- &lt;br&gt; --&gt;

&lt;!-- Function             | Description --&gt;
&lt;!-- -------------------- | ------------- --&gt;
&lt;!-- `future_apply()`     | apply function over array margin --&gt;
&lt;!-- `future_lapply()`    | apply function over list or vector --&gt;
&lt;!-- `future_sapply()`    | apply function over list or vector --&gt;
&lt;!-- `future_replicate()` | apply function over list or vector --&gt;
&lt;!-- `future_mapply()`    | apply function to multiple list or vector arguments --&gt;
&lt;!-- `future_Map()`       | apply function to multiple list or vector arguments --&gt;

&lt;!-- &lt;br&gt; --&gt;

&lt;!-- These functions operate sequentially by default but parallel computation is easily set up...  --&gt;

&lt;!-- --- --&gt;
&lt;!-- ## The `future.apply` Package --&gt;

&lt;!-- How to run in parallel: set `plan(multiprocess)` --&gt;

&lt;!-- &lt;br&gt; --&gt;

&lt;!-- ![:image 85%](../img/future_apply.png) --&gt;

&lt;!-- --- --&gt;
&lt;!-- ## The `future.apply()` Package --&gt;

&lt;!-- **Example: many normal sample means** --&gt;

&lt;!-- ```{r, cache=T} --&gt;
&lt;!-- tictoc::tic() --&gt;
&lt;!-- invisible( --&gt;
&lt;!--   sapply(rep(1e5, 10000), function(x) mean(rnorm(x))) --&gt;
&lt;!-- ) --&gt;
&lt;!-- tictoc::toc() --&gt;
&lt;!-- ``` --&gt;

&lt;!-- ```{r, cache=T} --&gt;
&lt;!-- tictoc::tic() --&gt;
&lt;!-- invisible( --&gt;
&lt;!--   future_sapply(rep(1e5, 10000), function(x) mean(rnorm(x))) --&gt;
&lt;!-- ) --&gt;
&lt;!-- tictoc::toc() --&gt;
&lt;!-- ``` --&gt;

---
## References

Gillespie, C. and Lovelace (2016), *R. Efficient R Programming*. O'Reilly Media.

Peng, Roger D. (2016). *R Programming for Data Science*. The bookdown Archive.

Wickham, H. (2019). *Advanced R*. 2nd Edition. Taylor &amp; Francis, CRC Press.

---
class: segue-red

![:image 20%](pikabye.gif)
### Thank You!
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="../assets/remark-zoom.js"></script>
<script src="../xaringan_files/macros.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<script type="text/x-mathjax-config">
  
  function addCopyButtonToCode(){
    // get all code elements
    var allCodeBlocksElements = $( ".r" );
    // For each element, do the following steps
    allCodeBlocksElements.each(function(ii) {
      // define a unique id for this element and add it
      var currentId = "codeblock" + (ii + 1);
      $(this).attr('id', currentId);
      

      // add the button just after the text in the code block w/ jquery
      var clipButton = '<img src="https://image.flaticon.com/icons/svg/54/54662.svg" width="16" alt="Copy to clipboard" onclick=copyToClipboard("#' + currentId + '") style="float:right;cursor:pointer;">';
      $(this).prepend(clipButton);
    });
    
  }

$(document).ready(function () {
  // Once the DOM is loaded for the page, attach clipboard buttons
  addCopyButtonToCode();
});


function copyToClipboard(element) {
  var $temp = $("<textarea>");
  $("body").append($temp);
  
  let vars = '';
  
  $(element).children().each((i, v) => {
      vars += $(v).text() + '\n'
    })
  
  $temp.val(vars).select();
  document.execCommand("copy");
  $temp.remove();
}

</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
